### 通过文件方式提供密码

debezium 可以通过配置 secret 的方式，避免将数据库密码等敏感信息明文配置到连接器中，比如，使用文件的方式配置，可以使用如下步骤：



1. 配置 worker，在 worker 中加入如下配置（重启生效）

   ```properties
   config.providers=file
   config.providers.file.class=org.apache.kafka.common.config.provider.FileConfigProvider
   ```

2. 配置连接器，连接器可以使用如下配置

   ```json
   {
       "name": "inventory-connector",
       "config": {
           "database.user": "${file:/secrets/mysql.properties:user}",
           "database.password": "${file:/secrets/mysql.properties:password}",
           ...
       }
   }
   ```

连接器启动时，会在 `/secrets/mysql.properties` 文件中获取 `user` 和 `password` 字段的值用来作为 `database.user` 和 `database.password` 的值。



### 通过自定义方式提供密码

如果需要自定义密码的提供方式，比如，只是将配置的内容简单反转即可，可以参考下面的步骤实现：

#### 新建 java 项目

创建目录结构和基本信息：

```shell
# 1. 创建项目根目录
mkdir kafka-custom-provider
cd kafka-custom-provider

# 2. 创建源码目录
mkdir -p src/main/java/com/example/kafka/connect
```

在根目录创建 `pom.xml` 并写入下面的内容，如果有其它依赖，可以根据需求添加：

```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" 
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.example.kafka.connect</groupId>
    <artifactId>custom-decrypting-provider</artifactId>
    <version>1.0.0</version>
    <packaging>jar</packaging>

    <properties>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <kafka.version>3.6.1</kafka.version> 
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>${kafka.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>connect-api</artifactId>
            <version>${kafka.version}</version>
            <scope>provided</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>3.3.0</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

新建代码文件 `src/main/java/com/example/kafka/connect/CustomDecryptingProvider.java`：

```java
package com.example.kafka.connect;

import org.apache.kafka.common.config.ConfigData;
import org.apache.kafka.common.config.provider.ConfigProvider;

import java.io.IOException;
import java.util.Collections;
import java.util.Map;
import java.util.Set;
import java.util.HashMap;

public class CustomDecryptingProvider implements ConfigProvider {

    private static final String DEFAULT_KEY = "value"; 

    @Override
    public void configure(Map<String, ?> configs) {
        // 在这里可以读取配置参数，例如解密算法密钥等。
    }

    /**
     * 核心方法：将 path 视为加密密文进行解密。
     */
    @Override
    public ConfigData get(String path) throws IOException {
        // path 参数现在就是连接器配置中传入的加密字符串
        String encryptedValue = path; 

        // 1. 调用自定义解密逻辑
        String decryptedValue = customDecrypt(encryptedValue);

        // 2. 将解密后的值放入 Map，使用固定键
        Map<String, String> data = new HashMap<>();
        data.put(DEFAULT_KEY, decryptedValue);

        // 3. 返回包含解密数据的 ConfigData
        return new ConfigData(data);
    }

    @Override
    public ConfigData get(String path, Set<String> keys) throws IOException {
        // 为了简化，可以直接调用上面的 get(path)
        return get(path); 
    }
    
    @Override
    public void close() throws IOException {
        // 清理资源
    }

    private String customDecrypt(String encryptedValue) {
        // 简单示例：反转字符串作为“解密”结果
        StringBuilder sb = new StringBuilder(encryptedValue);
        return sb.reverse().toString(); 
    }
}
```

创建服务配置文件：

```shell
# 1. 创建配置文件目录 
mkdir -p src/main/resources/META-INF/services

# 2. 将自定义的类路径写入到配置文件
echo "com.example.kafka.connect.CustomProvider" > src/main/resources/META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider
```

该操作的目的是为了让自定义的类可以被 kafka connect 发现，注意 **`src/main/resources/META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider` 的文件路径和文件名总是固定的**。

如果要提供多个类用于加载，可以在文件中写入多行内容，参考 [kafka-kubernetes-config-provider/src/main/resources/META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider at main · strimzi/kafka-kubernetes-config-provider](https://github.com/strimzi/kafka-kubernetes-config-provider/blob/main/src/main/resources/META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider)

打包：

```she
mvn clean package
```

在 kafka connect 服务器的 plugin.path 目录下创建一个名为 `custom-decrypting-provider` 的目录，并将打包好的 jar 包放到该目录下（如果有额外的依赖，使用带 `with-dependenies` 后缀的 jar，否则使用另外一个没有依赖的 jar 即可）。然后在 worker 配置中加入下面的配置：

```properties
config.providers=cust
config.providers.file.class=com.example.kafka.connect.CustomProvider
```

重启 worker，就可以使用自定义的 provider 了，示例如下：

```json
{
    "name": "inventory-connector",
    "config": {
        "database.password": "${cust:reverse-me:value}",
        ...
    }
}
```



### Reference

- [Secrets externalization with Debezium connectors](https://debezium.io/blog/2019/12/13/externalized-secrets/)
- [KIP-297: Externalizing Secrets for Connect Configurations - Apache Kafka - Apache Software Foundation](https://cwiki.apache.org/confluence/display/KAFKA/KIP-297%3A+Externalizing+Secrets+for+Connect+Configurations)
- [cp-kafka-connect: Custom ConfigProvider ClassNotFoundExceptions on startup · Issue #102 · confluentinc/kafka-images](https://github.com/confluentinc/kafka-images/issues/102)